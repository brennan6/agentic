import os
import json
import re
from uuid import uuid4
from dotenv import load_dotenv
import operator
from typing import TypedDict, List, Annotated
from pydantic import BaseModel, Field

# Load environment variables:
load_dotenv()

# Langgraph utils:
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import MessagesState
from langchain_core.messages import HumanMessage, ToolMessage, AIMessage

# Langchain utils:
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults


# Define state types
class State(TypedDict):
    """
    Represents the state of the shopping agent graph.

    Attributes:
        session_id (str): A unique identifier for the conversation session.
        request (dict): The original user request data.
        messages (Annotated[List[dict], operator.add]): The sequence of messages,
            where new messages are added to the end of the list.
    """
    session_id: str
    request: dict
    messages: Annotated[List[dict], operator.add]


class ShoppingAgent:
    """ An agent that researches product prices and notifies of purchase opportunities. """

    def __init__(self, research_model, response_model,
                 limit_price, tools):
        """
        Initializes the shopping agent.

        Args:
            research_model: The language model used for research and tool use.
            response_model: The language model used for generating the final output.
            limit_price (float or int): The price limit to check against.
            tools (list): A list of tools available to the agent.
        """
        self.research_model = research_model
        self.response_model = response_model

        self.graph = self._build_graph(tools)

        self.limit_price = limit_price

    def _build_graph(self, tools) -> StateGraph:
        """
        Builds and compiles the agent's StateGraph workflow.

        Args:
            tools (list): A list of tools to bind to the research model.

        Returns:
            StateGraph: A compiled LangGraph object ready for execution.
        """
        graph = StateGraph(State)

        # Define Nodes:
        graph.add_node("llm", self._call_llm)
        graph.add_node("shop", self._search)
        graph.add_node("process_shop", self._call_llm)
        graph.add_node("notify", self._notify)
        graph.add_node("output", self._output)

        # Define start:
        graph.set_entry_point("llm")

        # Define Edges:
        graph.add_edge("llm", "shop")
        graph.add_edge("shop", "process_shop")
        graph.add_conditional_edges(
            "process_shop",
            self._exists_deal,
            {True: "notify", False: END}
        )
        graph.add_edge("notify", "output")
        graph.add_edge("output", END)

        # Assign tools: 
        self.tools = {t.name: t for t in tools}
        self.research_model = self.research_model.bind_tools(tools)

        return graph.compile()

    def _call_llm(self, state: State) -> dict:
        """
        Invokes the research language model with the current messages.

        Args:
            state (State): The current state of the graph.

        Returns:
            dict: A dictionary containing the model's response message.
        """
        return {"messages": [self.research_model.invoke(state["messages"])]}

    def _output(self, state: State) -> dict:
        """
        Generates the final structured response for the user.

        Args:
            state (State): The current state of the graph.

        Returns:
            dict: A dictionary containing the final response message.
        """
        final_response = self.response_model.invoke(state["messages"])

        return {"messages": [final_response]}

    def _search(self, state: State) -> dict:
        """
        Executes tool calls generated by the LLM, primarily for searching.

        Args:
            state (State): The current state of the graph.

        Returns:
            dict: A dictionary containing ToolMessage objects with search results.
        """
        tool_calls = state['messages'][-1].tool_calls
        results = []
        for t in tool_calls:
            result = self.tools[t['name']].invoke(t['args'])
            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))

        return {'messages': results}

    def _notify(self, state: State) -> dict:
        """
        Executes the notification tool if the price is below the user's limit.

        Args:
            state (State): The current state of the graph.

        Returns:
            dict: A dictionary with a ToolMessage indicating notification status.
        """
        tool = state['messages'][-1].tool_calls[0] if state['messages'][-1].tool_calls else None
        rejection_str = "Did not notify user."

        if tool is not None:
            print("Tool comparison price:", tool["args"]["price"])
            if tool["args"]["price"] < int(self.limit_price):
                result = self.tools[tool['name']].invoke(tool['args'])
                return {'messages': [ToolMessage(tool_call_id=tool['id'], name=tool['name'], content=str(result))]}
            else:
                print("Made it here.")
                return {'messages': [ToolMessage(tool_call_id=tool['id'], name=tool['name'], content=rejection_str)]}
        else:
            return {'messages': [ToolMessage(tool_call_id=tool['id'], name=tool['name'], content=rejection_str)]}

    def _exists_deal(self, state: State) -> bool:
        """
        Determines if a deal was found, routing the graph to notify or end.

        Args:
            state (State): The current state of the graph.

        Returns:
            bool: True if a tool call exists in the last message, otherwise False.
        """
        tool = state['messages'][-1].tool_calls[0] if state['messages'][-1].tool_calls else None

        if tool is not None:
            return True
        else:
            return False

    def _execute(self, user_prompt: str, request: dict):
        """
        Runs the agent graph with the user's prompt and request data.

        Args:
            user_prompt (str): The initial prompt detailing the user's goal.
            request (dict): The dictionary of user-provided data.

        Returns:
            dict: The final output state from the graph execution.
        """
        session_id = str(uuid4())
        initial_state = {"session_id": session_id,
                         "messages": [HumanMessage(content=user_prompt)],
                         "request": request}
        return self.graph.invoke(initial_state)